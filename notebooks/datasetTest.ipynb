{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cafd535",
   "metadata": {},
   "source": [
    "# ACT-X Dataset Consistency Check\n",
    "\n",
    "이 노트북은 ACT-X 원본 JSON과 변환된 `*_llada.json`을 각각 로드한 뒤, \n",
    "새로 설계한 FIM 레이블 조립 규칙이 두 데이터셋에서 동일한 결과를 만드는지 검증합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51373767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "BASE_DIR = Path('/home/20223206/ACT-X')\n",
    "TOKENIZER_DIR = Path('/home/20223206/LLaDA-V/train/llada_v_prepare/files')\n",
    "QUESTION_TEMPLATE = \"What activity is happening in this image?\"\n",
    "ANSWER_BLOCK_SIZE = 20\n",
    "\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR, trust_remote_code=True)\n",
    "RESERVED_TOKEN = '<|reserved_token_1|>'\n",
    "RESERVED_TOKEN_ID = tokenizer.convert_tokens_to_ids(RESERVED_TOKEN)\n",
    "print('Tokenizer:', tokenizer.__class__.__name__)\n",
    "print('Reserved token id:', RESERVED_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_actx(path: Path) -> List[Dict]:\n",
    "    payload = json.load(path.open())\n",
    "    if isinstance(payload, list):\n",
    "        return payload\n",
    "    if isinstance(payload, dict):\n",
    "        return [dict(id=k, **v) for k, v in payload.items()]\n",
    "    raise ValueError(f'Unsupported payload type: {type(payload)}')\n",
    "\n",
    "\n",
    "def expand_actx_records(raw_records: List[Dict], image_prefix: str = 'images') -> List[Dict]:\n",
    "    expanded = []\n",
    "    for entry in raw_records:\n",
    "        base_id = entry.get('image_id') or entry.get('id')\n",
    "        answer = (entry.get('answers') or entry.get('answer') or '').strip()\n",
    "        explanations = entry.get('explanation') or ['']\n",
    "        if not isinstance(explanations, list):\n",
    "            explanations = [explanations]\n",
    "        cleaned = [exp.strip() for exp in explanations if exp and exp.strip()]\n",
    "        if not cleaned:\n",
    "            cleaned = ['']\n",
    "\n",
    "        for idx, exp in enumerate(cleaned):\n",
    "            conversations = [\n",
    "                {'from': 'human', 'value': f\"<image>\\n{QUESTION_TEMPLATE}\"},\n",
    "                {'from': 'gpt', 'value': ''},\n",
    "            ]\n",
    "            expanded.append(\n",
    "                {\n",
    "                    'id': base_id if idx == 0 else f\"{base_id}_{idx}\",\n",
    "                    'image': str(Path(image_prefix) / entry.get('image_name', f\"{base_id}.jpg\")),\n",
    "                    'answer': answer,\n",
    "                    'explanation': exp,\n",
    "                    'explanation_index': idx,\n",
    "                    'conversations': conversations,\n",
    "                }\n",
    "            )\n",
    "    return expanded\n",
    "\n",
    "\n",
    "def build_fim_label(answer: str, explanation: str) -> Tuple[List[int], str]:\n",
    "    answer = (answer or '').strip()\n",
    "    prefix_ids = tokenizer('The answer is ', add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False).input_ids if answer else []\n",
    "\n",
    "    block_ids = [RESERVED_TOKEN_ID] * ANSWER_BLOCK_SIZE\n",
    "    copy_len = min(len(prefix_ids), ANSWER_BLOCK_SIZE)\n",
    "    block_ids[:copy_len] = prefix_ids[:copy_len]\n",
    "\n",
    "    remaining = ANSWER_BLOCK_SIZE - copy_len\n",
    "    if remaining > 0 and answer_ids:\n",
    "        ans_copy_len = min(len(answer_ids), remaining)\n",
    "        block_ids[copy_len:copy_len + ans_copy_len] = answer_ids[:ans_copy_len]\n",
    "\n",
    "    explanation = (explanation or '').strip()\n",
    "    explanation_ids: List[int] = []\n",
    "    if explanation:\n",
    "        if explanation[-1] not in '.!?':\n",
    "            explanation = explanation + '.'\n",
    "        because_ids = tokenizer(' because', add_special_tokens=False).input_ids\n",
    "        expl_ids = tokenizer(explanation, add_special_tokens=False).input_ids\n",
    "        explanation_ids = because_ids + expl_ids\n",
    "\n",
    "    label_ids = block_ids + explanation_ids\n",
    "    label_text = tokenizer.decode(label_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    return label_ids, label_text\n",
    "\n",
    "\n",
    "def attach_labels(records: List[Dict]) -> List[Dict]:\n",
    "    labeled = []\n",
    "    for rec in records:\n",
    "        _, label_text = build_fim_label(rec.get('answer'), rec.get('explanation'))\n",
    "        new_rec = deepcopy(rec)\n",
    "        new_rec['conversations'] = deepcopy(rec['conversations'])\n",
    "        new_rec['conversations'][1]['value'] = label_text\n",
    "        labeled.append(new_rec)\n",
    "    return labeled\n",
    "\n",
    "\n",
    "def compare_datasets(baseline: List[Dict], target: List[Dict]) -> Dict:\n",
    "    def index(records):\n",
    "        return {\n",
    "            (rec['id'], rec.get('explanation_index', 0)): rec\n",
    "            for rec in records\n",
    "        }\n",
    "\n",
    "    base_idx = index(baseline)\n",
    "    target_idx = index(target)\n",
    "\n",
    "    missing = set(base_idx) - set(target_idx)\n",
    "    extra = set(target_idx) - set(base_idx)\n",
    "\n",
    "    mismatched = []\n",
    "    for key in sorted(set(base_idx).intersection(target_idx)):\n",
    "        if base_idx[key]['conversations'][1]['value'] != target_idx[key]['conversations'][1]['value']:\n",
    "            mismatched.append((key, base_idx[key]['conversations'][1]['value'], target_idx[key]['conversations'][1]['value']))\n",
    "\n",
    "    return {\n",
    "        'baseline_count': len(base_idx),\n",
    "        'target_count': len(target_idx),\n",
    "        'missing': missing,\n",
    "        'extra': extra,\n",
    "        'mismatched': mismatched,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bf58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'train': {\n",
    "        'raw': BASE_DIR / 'actX_train.json',\n",
    "        'converted': BASE_DIR / 'actX_train_llada.json',\n",
    "    },\n",
    "    'test': {\n",
    "        'raw': BASE_DIR / 'actX_test.json',\n",
    "        'converted': BASE_DIR / 'actX_test_llada.json',\n",
    "    },\n",
    "}\n",
    "\n",
    "comparison_summary = {}\n",
    "for split, paths in datasets.items():\n",
    "    raw_records = load_raw_actx(paths['raw'])\n",
    "    expanded_raw = expand_actx_records(raw_records)\n",
    "    expanded_converted = json.load(paths['converted'].open())\n",
    "\n",
    "    labeled_raw = attach_labels(expanded_raw)\n",
    "    labeled_converted = attach_labels(expanded_converted)\n",
    "\n",
    "    cmp = compare_datasets(labeled_raw, labeled_converted)\n",
    "    comparison_summary[split] = cmp\n",
    "\n",
    "    print(f\"[{split}] baseline={cmp['baseline_count']} target={cmp['target_count']} mismatched={len(cmp['mismatched'])}\")\n",
    "    if cmp['missing']:\n",
    "        print('  Missing keys:', list(cmp['missing'])[:3])\n",
    "    if cmp['extra']:\n",
    "        print('  Extra keys:', list(cmp['extra'])[:3])\n",
    "    if cmp['mismatched']:\n",
    "        key, base_val, target_val = cmp['mismatched'][0]\n",
    "        print('  Example mismatch:', key)\n",
    "\n",
    "comparison_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c122e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a couple of assembled labels\n",
    "sample_ids = ['020934932', '008364309_1', '026558760']\n",
    "for split, paths in datasets.items():\n",
    "    records = json.load(paths['converted'].open())\n",
    "    indexed = {\n",
    "        (rec['id'], rec.get('explanation_index', 0)): rec\n",
    "        for rec in records\n",
    "    }\n",
    "    print(f\"=== {split.upper()} ===\")\n",
    "    for sid in sample_ids:\n",
    "        matches = [rec for key, rec in indexed.items() if key[0] == sid]\n",
    "        if not matches:\n",
    "            continue\n",
    "        labeled_matches = attach_labels(matches)\n",
    "        for rec in labeled_matches:\n",
    "            idx = rec.get('explanation_index', 0)\n",
    "            print(f\"{sid} (exp #{idx}) -> {rec['conversations'][1]['value']}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}